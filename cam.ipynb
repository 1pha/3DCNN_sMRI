{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T02:21:56.609406Z",
     "start_time": "2020-12-10T02:21:52.882559Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "from itertools import chain\n",
    "\n",
    "# for reading and displaying images\n",
    "# from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "# for creating validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "\n",
    "# models\n",
    "from src.architectures.resnet import *\n",
    "from src.architectures.levakov_96 import *\n",
    "from src.architectures.inception import *\n",
    "from src.architectures.dinsdale import *\n",
    "\n",
    "from src.run import *\n",
    "\n",
    "from torch.optim import *\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# CAM\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from cam import CAM, GradCAM, GradCAMpp, SmoothGradCAMpp\n",
    "from utils.visualize import visualize, reverse_normalize\n",
    "from utils.imagenet_labels import label2idx, idx2label\n",
    "\n",
    "from medcam import medcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "for kFold, it will be run again on the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T02:21:56.864904Z",
     "start_time": "2020-12-10T02:21:56.610333Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.dataloader import *\n",
    "\n",
    "train_dset = MyDataset(task_type='age')\n",
    "test_dset = MyDataset(task_type='age', test=True)\n",
    "\n",
    "train_loader = DataLoader(train_dset, batch_size=8)\n",
    "test_loader = DataLoader(test_dset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T02:21:56.880935Z",
     "start_time": "2020-12-10T02:21:56.865946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Levakov is selected.\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm3d-1        [-1, 1, 96, 96, 96]               2\n",
      "            Conv3d-2        [-1, 8, 94, 94, 94]             224\n",
      "              ReLU-3        [-1, 8, 94, 94, 94]               0\n",
      "            Conv3d-4        [-1, 8, 92, 92, 92]           1,736\n",
      "              ReLU-5        [-1, 8, 92, 92, 92]               0\n",
      "         MaxPool3d-6        [-1, 8, 46, 46, 46]               0\n",
      "       BatchNorm3d-7        [-1, 8, 46, 46, 46]              16\n",
      "            Conv3d-8       [-1, 16, 44, 44, 44]           3,472\n",
      "              ReLU-9       [-1, 16, 44, 44, 44]               0\n",
      "           Conv3d-10       [-1, 16, 42, 42, 42]           6,928\n",
      "             ReLU-11       [-1, 16, 42, 42, 42]               0\n",
      "        MaxPool3d-12       [-1, 16, 21, 21, 21]               0\n",
      "      BatchNorm3d-13       [-1, 16, 21, 21, 21]              32\n",
      "          Dropout-14       [-1, 16, 21, 21, 21]               0\n",
      "           Conv3d-15       [-1, 32, 19, 19, 19]          13,856\n",
      "             ReLU-16       [-1, 32, 19, 19, 19]               0\n",
      "           Conv3d-17       [-1, 32, 17, 17, 17]          27,680\n",
      "             ReLU-18       [-1, 32, 17, 17, 17]               0\n",
      "        MaxPool3d-19          [-1, 32, 8, 8, 8]               0\n",
      "      BatchNorm3d-20          [-1, 32, 8, 8, 8]              64\n",
      "          Dropout-21          [-1, 32, 8, 8, 8]               0\n",
      "           Conv3d-22          [-1, 64, 6, 6, 6]          55,360\n",
      "             ReLU-23          [-1, 64, 6, 6, 6]               0\n",
      "           Conv3d-24          [-1, 64, 4, 4, 4]         110,656\n",
      "             ReLU-25          [-1, 64, 4, 4, 4]               0\n",
      "        MaxPool3d-26          [-1, 64, 2, 2, 2]               0\n",
      "      BatchNorm3d-27          [-1, 64, 2, 2, 2]             128\n",
      "          Dropout-28          [-1, 64, 2, 2, 2]               0\n",
      "           Linear-29                  [-1, 256]         131,328\n",
      "          Dropout-30                  [-1, 256]               0\n",
      "           Linear-31                  [-1, 128]          32,896\n",
      "          Dropout-32                  [-1, 128]               0\n",
      "           Linear-33                    [-1, 1]             129\n",
      "================================================================\n",
      "Total params: 384,507\n",
      "Trainable params: 384,507\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.38\n",
      "Forward/backward pass size (MB): 263.77\n",
      "Params size (MB): 1.47\n",
      "Estimated Total Size (MB): 268.61\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def load_model(model, verbose=True):\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Model {model.capitalize()} is selected.')\n",
    "\n",
    "    if model == 'resnet':\n",
    "        \n",
    "\n",
    "        opt = Option()\n",
    "        model = generate_model(model_depth=opt.model_depth,\n",
    "                                    n_classes=opt.n_classes,\n",
    "                                    n_input_channels=opt.n_input_channels,\n",
    "                                    shortcut_type=opt.shortcut_type,\n",
    "                                    conv1_t_size=opt.conv1_t_size,\n",
    "                                    conv1_t_stride=opt.conv1_t_stride,\n",
    "                                    no_max_pool=opt.no_max_pool,\n",
    "                                    widen_factor=opt.resnet_widen_factor)\n",
    "\n",
    "    elif model == 'levakov':\n",
    "        model = Levakov(task_type='age')\n",
    "\n",
    "    elif model == 'inception':\n",
    "        model = Inception3()\n",
    "\n",
    "    elif model == 'dinsdale':\n",
    "        model = Dinsdale(1, 1, 2)\n",
    "\n",
    "    else: return None\n",
    "\n",
    "    model.to(device)\n",
    "    if verbose:\n",
    "        print(summary(model, input_size=(1, 96, 96, 96)))\n",
    "    \n",
    "    return model, device\n",
    "\n",
    "model = 'levakov'\n",
    "model, device = load_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T02:21:56.896549Z",
     "start_time": "2020-12-10T02:21:56.881937Z"
    }
   },
   "outputs": [],
   "source": [
    "task_type = 'age' # no longer used.\n",
    "resize = True\n",
    "scheduler = False\n",
    "epochs = range(100)\n",
    "folds = range(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T02:22:13.892761Z",
     "start_time": "2020-12-10T02:22:13.882674Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.losses import RMSELoss\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "lamb = 0.0005\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T02:22:14.551562Z",
     "start_time": "2020-12-10T02:22:14.546564Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_fn = RMSELoss()\n",
    "mae_fn = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pha\\anaconda3\\envs\\1pha\\lib\\site-packages\\torch\\serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv3d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\pha\\anaconda3\\envs\\1pha\\lib\\site-packages\\torch\\serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm3d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\pha\\anaconda3\\envs\\1pha\\lib\\site-packages\\torch\\serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\pha\\anaconda3\\envs\\1pha\\lib\\site-packages\\torch\\serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool3d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\pha\\anaconda3\\envs\\1pha\\lib\\site-packages\\torch\\serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\pha\\anaconda3\\envs\\1pha\\lib\\site-packages\\torch\\serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.AdaptiveAvgPool3d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\pha\\anaconda3\\envs\\1pha\\lib\\site-packages\\torch\\serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv3d(1, 16, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n",
       "  (bn1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (fc1): Linear(in_features=6912, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc4): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('models/2020-12-04_2113_9.632_model.pth')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = medcam.inject(model, output_dir=\"attentio_maps\", save_maps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the target layer you want to visualize\n",
    "target_layer = model.layer4[0].conv2\n",
    "\n",
    "# wrapper for class activation mapping. Choose one of the following.\n",
    "# wrapped_model = CAM(model, target_layer)\n",
    "# wrapped_model =GradCAM(model, target_layer)\n",
    "# wrapped_model = GradCAMpp(model, target_layer)\n",
    "wrapped_model = SmoothGradCAMpp(model, target_layer, n_samples=25, stdev_spread=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, y) in enumerate(train_loader):\n",
    "    \n",
    "    if i < 2:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    else:\n",
    "        break\n",
    "        \n",
    "tensor = x[0][None, ...].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing. mean and std from ImageNet\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485],\n",
    "   std=[0.229]\n",
    ")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected tensor to be a tensor image of size (C, H, W). Got tensor.size() = torch.Size([1, 141, 172, 110]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-6e5deaa48f81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# convert image to tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# reshape 4D tensor (N, C, H, W)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\1pha\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\1pha\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \"\"\"\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\1pha\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m         raise ValueError('Expected tensor to be a tensor image of size (C, H, W). Got tensor.size() = '\n\u001b[1;32m--> 284\u001b[1;33m                          '{}.'.format(tensor.size()))\n\u001b[0m\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected tensor to be a tensor image of size (C, H, W). Got tensor.size() = torch.Size([1, 141, 172, 110])."
     ]
    }
   ],
   "source": [
    "# convert image to tensor\n",
    "tensor = preprocess(x[0])\n",
    "\n",
    "# reshape 4D tensor (N, C, H, W)\n",
    "tensor = tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class ids 0\t probability 1.0\n"
     ]
    }
   ],
   "source": [
    "cam, idx = wrapped_model(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 9, 6, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ee45e4f248>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize only cam\n",
    "imshow(cam.squeeze().cpu().numpy(), alpha=0.5, cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse normalization for display\n",
    "img = reverse_normalize(tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-6lylwdcz\\opencv\\modules\\imgproc\\src\\colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'cv::colormap::ColorMap::operator ()'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-0fb868504c50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mheatmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mG:\\내 드라이브\\brain_data\\workspace\\3d_brain\\utils\\visualize.py\u001b[0m in \u001b[0;36mvisualize\u001b[1;34m(img, cam)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mcam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bilinear'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mcam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m255\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mheatmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplyColorMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLORMAP_JET\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mheatmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mheatmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheatmap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-6lylwdcz\\opencv\\modules\\imgproc\\src\\colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'cv::colormap::ColorMap::operator ()'\n"
     ]
    }
   ],
   "source": [
    "heatmap = visualize(img, cam[0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedBackpropRelu(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        input = ctx.saved_tensors[0]\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[grad_input<0] = 0\n",
    "        grad_input[input<0]=0\n",
    "        return grad_input\n",
    "     \n",
    "guided_relu = GuidedBackpropRelu.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
